{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiowanie strukur do wykorzystania przy tworzeniu grafu\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end #dziedziczenie po grafie, wykorzystywany do rÃ³Å¼nych operacji\n",
    "\n",
    "#staÅ‚a w grafie, output przechowywuje jej wartoÅ›Ä‡, parametryczna stuktura\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "#zmienna, gradient przechowuje pochodnÄ… po pewnej wartoÅ›ci\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "#skalar input jest zamieniany na skalar output\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "#przyjmuje input o dowolnym wymiarze, przydatny potem do sledzenia rÃ³Å¼nych wersji forward i backward\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty-printing\n",
    "It helps tracking what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show (generic function with 286 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drukowanie graphu, io - input output stream\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n â”£â” ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n â”—â” âˆ‡ \");  summary(io, x.gradient)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building\n",
    "At first we have a set of loosely-coupled graph nodes. The following procedures build a proper graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topological_sort (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function visit(node::GraphNode, visited, order)\n",
    "    if node âˆˆ visited\n",
    "    else\n",
    "        push!(visited, node) #dodanie do listy z juÅ¼ odwiedzonymi wÄ™zÅ‚ami\n",
    "        push!(order, node) #utworzenie listy ze wszystkimi wÄ™zÅ‚ami\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node âˆˆ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode) #sortowanie elemntÃ³w przechowane w \n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order) #head to poczÄ…tek graphu\n",
    "    return order #zwraca posortowane elementy tablicy w postaci array\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "#wyzerowanie pochodnej dla rÃ³Å¼nych typÃ³w\n",
    "\n",
    "compute!(node::Constant) = nothing #wartoÅ›ci sÄ… juÅ¼ wiadome\n",
    "compute!(node::Variable) = nothing #wartoÅ›ci sÄ… juÅ¼ obliczone\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "#wywoÅ‚anie rÃ³Å¼nych funkcji forward okreÅ›lonych dla rÃ³Å¼nych operatorÃ³w\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node) #obliczenie wartoÅ›ci dla konkretnego wÄ™zÅ‚a\n",
    "        reset!(node) #wyzerowanie pochodnych\n",
    "    end\n",
    "    return last(order).output #ostatni element w liÅ›cie, koniec graphu\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "#aktualizacja wartoÅ›ci gradientu dla danego wÄ™zÅ‚a lub jego inicjalizacja\n",
    "\n",
    "function backward!(order::Vector; seed=1.0) #wejscie to vector posortowanych wÄ™zÅ‚Ã³w\n",
    "    result = last(order)\n",
    "    result.gradient = seed #zapoczÄ…tkowanie obliczeÅ„\n",
    "    @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    for node in reverse(order) #odwrÃ³cenie kolejnoÅ›ci przetrzymywanej w order i wykonanie backward pass od tyÅ‚u\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient) #zastosowanie funkcji backward dla zdefiniowanej operacji\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented operations\n",
    "Below is the list of supported operations on graph nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasted operators\n",
    "The operations act on vectors of values so, the gradients are computed as vector-jacobian-products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: *\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "    ðŸ = ones(length(node.output))\n",
    "    Jx = diagm(y .* ðŸ)\n",
    "    Jy = diagm(x .* ðŸ)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 3 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x, y) = return x .- y\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, y, g) = tuple(g,-g) #gradient x, gradient y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = tuple(g, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 5 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    ðŸ = ones(length(x))\n",
    "    J = ðŸ'\n",
    "    tuple(J' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 6 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x, y::Real, g) = let\n",
    "    ðŸ = ones(length(node.output))\n",
    "    Jx = diagm(ðŸ ./ y)\n",
    "    Jy = (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 7 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: max\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x, y) = return max.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(max)}, x, y, g) = let\n",
    "    Jx = diagm(isless.(y, x))\n",
    "    Jy = diagm(isless.(x, y))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 8 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(exp, x::GraphNode) = BroadcastedOperator(exp, x)\n",
    "forward(::BroadcastedOperator{typeof(exp)}, x) = return exp.(x)\n",
    "backward(node::BroadcastedOperator{typeof(exp)}, x, g) = let\n",
    "    grad = exp.(x) .* g\n",
    "    tuple(grad)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 9 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.Broadcast.broadcasted(^, x::GraphNode, y::GraphNode) = BroadcastedOperator(^, x, y)\n",
    "forward(::BroadcastedOperator{typeof(^)}, x, y) = return x .^ y\n",
    "backward(node::BroadcastedOperator{typeof(^)}, x, y, g) = let\n",
    "    ðŸ = ones(length(node.output))\n",
    "    Jx = y .* (x .^ (y .- ðŸ))\n",
    "    Jy = (x .^ y) .* log.(abs.(x))\n",
    "    tuple(Jx * g, Jy * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 10 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(x::GraphNode) = BroadcastedOperator(linear, x)\n",
    "forward(::BroadcastedOperator{typeof(linear)}, x) = return x\n",
    "backward(::BroadcastedOperator{typeof(linear)}, x, g) = tuple(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 11 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ïƒ(x::GraphNode) = BroadcastedOperator(Ïƒ, x)\n",
    "forward(::BroadcastedOperator{typeof(Ïƒ)}, x) = return 1 ./ (1 .+ exp.(-x))\n",
    "backward(node::BroadcastedOperator{typeof(Ïƒ)}, x, g) = let\n",
    "    y = node.output\n",
    "    dx = g .* y .* (1 .- y)\n",
    "    tuple(dx)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 12 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(img::GraphNode, ker::GraphNode) = BroadcastedOperator(conv, img, ker)\n",
    "forward(::BroadcastedOperator{typeof(conv)}, img, ker) = let\n",
    "    PAD = floor(size(ker)[1]/2)\n",
    "    PAD = Int64(PAD)\n",
    "    n, m = (28,28) .- PAD\n",
    "    J= zeros(28,28)\n",
    "    for i=(PAD+1):n, j=(PAD+1):m\n",
    "        J[i, j] = sum(img[(i-PAD):(i+PAD), (j-PAD):(j+PAD)] .* ker) \n",
    "    end\n",
    "    return J\n",
    "end\n",
    "\n",
    "backward(::BroadcastedOperator{typeof(conv)}, img, ker, g) = let #g to 28x28\n",
    "    PAD = floor(size(ker)[1]/2)\n",
    "    PAD = Int64(PAD)\n",
    "    dLdimg = conv(g, rot180(ker)) #po 28x28\n",
    "    dLdker = conv(rot180(img), g) #po 5x5\n",
    "    tuple(dLdimg[(PAD+1):(end-PAD), (PAD+1):(end-PAD)], dLdker)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv (generic function with 2 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uniwersalna wersja funkcji konwolucji\n",
    "function conv(IMG, KER)\n",
    "    PAD = floor(size(KER)[1]/2)\n",
    "    PAD = Int64(PAD) \n",
    "    n, m = size(IMG) .- PAD\n",
    "    J = zeros(size(IMG)) #bez przeskalowania\n",
    "    for i=(PAD+1):n, j=(PAD+1):m #skarjne piksele\n",
    "        J[i, j] = sum(IMG[(i-PAD):(i+PAD), (j-PAD):(j+PAD)] .* KER) \n",
    "    end\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 13 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgpool(img::GraphNode, ker_size::GraphNode) = BroadcastedOperator(avgpool, img, ker_size)\n",
    "forward(::BroadcastedOperator{typeof(avgpool)}, img, ker_size) = let\n",
    "    n, m = (28, 28) .- ker_size .+ 1\n",
    "    dim = Int64(28 / ker_size)\n",
    "    J = zeros(dim,dim)\n",
    "    for i=1:dim, j=1:dim\n",
    "        J[i, j] = sum(img[((i-1)*ker_size+1):(i*ker_size), ((j-1)*ker_size+1):(j*ker_size)])/(ker_size^2)\n",
    "    end\n",
    "    return J\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(avgpool)}, img, ker_size, g) = let \n",
    "    n = Int64(size(img)[1])\n",
    "    m = n\n",
    "    pooled_n = Int64(size(g)[1])\n",
    "    pooled_m = pooled_n\n",
    "    J = zeros(n, m)\n",
    "        for i=1:pooled_n\n",
    "            for j=1:pooled_m\n",
    "                i_start = (i - 1) * ker_size + 1\n",
    "                i_end = min(i_start + ker_size - 1, n)\n",
    "                j_start = (j - 1) * ker_size + 1\n",
    "                j_end = min(j_start + ker_size - 1, m)\n",
    "                pool_size = (i_end - i_start + 1) * (j_end - j_start + 1)\n",
    "                J[i_start:i_end, j_start:j_end] = (g[i, j] * ones(i_end - i_start + 1, j_end - j_start + 1)) / pool_size\n",
    "            end\n",
    "        end\n",
    "    tuple(J)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 14 methods)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zamiana macierzy na wektor, rozmiary wyniakjÄ… z avg poolingu z parametrem 2, dlatego zmniejszone 2krotnie\n",
    "flatten(img::GraphNode) = BroadcastedOperator(flatten, img)\n",
    "forward(::BroadcastedOperator{typeof(flatten)}, img) = return reshape(img,196)\n",
    "backward(::BroadcastedOperator{typeof(flatten)}, img, g) = let \n",
    "    dimg = reshape(g, (14,14))\n",
    "    tuple(dimg)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simplest multilayer-perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "\n",
    "trainX = MNIST(split=:train).features\n",
    "trainX = permutedims(trainX[:,:,:], (2,1,3)) #odwrÃ³cenie macierzy, numery odpowiadajÄ… pojedyÅ„cze wymiary\n",
    "trainY = MNIST(split=:train).targets\n",
    "\n",
    "testX = MNIST(split=:test).features\n",
    "testX = permutedims(testX[:,:,:], (2,1,3))\n",
    "testY = MNIST(split=:test).targets\n",
    "\n",
    "NROWS, NCOLS = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "return_prediction (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warstwy fully conected\n",
    "function dense(w, b, x, activation) return activation(w * x .+ b) end\n",
    "function dense(w, x, activation) return activation(w * x) end\n",
    "function dense(w, x) return w * x end\n",
    "\n",
    "#funkcja kosztu\n",
    "function mean_squared_loss(y, Å·)\n",
    "    return Constant(0.5) .* (y .- Å·) .^ Constant(2)\n",
    "end\n",
    "\n",
    "#zwrÃ³cenie wartoÅ›ci liczbowej predykcji\n",
    "function return_prediction(y)\n",
    "    return y.-Constant(0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function net(x, w1, w2, w3, y) #2x2 macierz obrazu, wagi warstw, label\n",
    "    aÌ‚ = conv(x, Variable(0.1.*ones(5,5))) #staÅ‚y filtr\n",
    "    aÌ‚.name = \"aÌ‚\"\n",
    "    bÌ‚ = avgpool(aÌ‚, Variable(2))\n",
    "    bÌ‚.name = \"bÌ‚\"\n",
    "    cÌ‚ = flatten(bÌ‚)\n",
    "    cÌ‚.name = \"cÌ‚\"\n",
    "    \n",
    "    xÌ‚ = dense(w1, cÌ‚, Ïƒ)\n",
    "    xÌ‚.name = \"xÌ‚\"\n",
    "    zÌ‚ = dense(w2, xÌ‚, Ïƒ)\n",
    "    zÌ‚.name = \"zÌ‚\" \n",
    "    Å· = dense(w3, zÌ‚)\n",
    "    Å·.name = \"Å·\"\n",
    "    E = mean_squared_loss(y, Å·)\n",
    "    E.name = \"loss\"\n",
    "    return topological_sort(E) #kom\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var w3\n",
       " â”£â” ^ 1Ã—10 Matrix{Float64}\n",
       " â”—â” âˆ‡ Nothing"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nadanie poczÄ…tkowych wag\n",
    "function he_init(fan_in, fan_out)\n",
    "    std_dev = sqrt(2 / fan_in)\n",
    "    weights = randn(Float32, fan_in, fan_out) * std_dev\n",
    "    return weights\n",
    "end\n",
    "\n",
    "W1  = Variable(he_init(100,196), name=\"w1\")\n",
    "W2  = Variable(he_init(10, 100), name=\"w2\")\n",
    "W3  = Variable(he_init(1,10), name=\"w3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#czemu tu jest potrzebna inicjalizacja grafu ?\n",
    "x = Variable(trainX[:,:,65], name=\"x\")\n",
    "y = Variable(trainY[65], name=\"y\")\n",
    "graph = net(x,W1,W2,W3,y)\n",
    "forward!(graph)\n",
    "backward!(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Current loss: 1.20190092765396"
     ]
    }
   ],
   "source": [
    "import Statistics: mean\n",
    "\n",
    "epochs=5\n",
    "training_set_size = 60000\n",
    "\n",
    "#inicjacja tablicy na funkjcÄ™ strat\n",
    "losses = Float64[] \n",
    "\n",
    "for i=1:epochs\n",
    "    for j=1:training_set_size\n",
    "        x = Variable(trainX[:,:,j], name=\"x\") #czemu tu i niÅ¼ej to jest jako variable?\n",
    "        y = Variable(trainY[j], name=\"y\")\n",
    "        graph = net(x, W1, W2, W3, y)\n",
    "        currentloss = forward!(graph) #pierwsze przejÅ›cie potrzebne do inicjalizacji, czemu ?\n",
    "        backward!(graph)\n",
    "        if (i < 5) #kom\n",
    "            W1.output -= 0.001W1.gradient\n",
    "            W2.output -= 0.001W2.gradient\n",
    "            W3.output -= 0.001W3.gradient\n",
    "        else\n",
    "            W1.output -= 0.01W1.gradient\n",
    "            W2.output -= 0.01W2.gradient\n",
    "            W3.output -= 0.01W3.gradient\n",
    "        end\n",
    "        push!(losses, first(currentloss))\n",
    "    end\n",
    "println(\"Epoch: \", i)\n",
    "println(\"Current loss: \", mean(losses[training_set_size*(i-1)+1:training_set_size*i]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(x, w1, w2, w3)\n",
    "    aÌ‚ = conv(x, Variable(0.1.*ones(5,5)))\n",
    "    aÌ‚.name = \"aÌ‚\"\n",
    "    bÌ‚ = avgpool(aÌ‚, Variable(2))\n",
    "    bÌ‚.name = \"bÌ‚\"\n",
    "    cÌ‚ = flatten(bÌ‚)\n",
    "    cÌ‚.name = \"cÌ‚\"\n",
    "    xÌ‚ = dense(w1, cÌ‚, Ïƒ)\n",
    "    xÌ‚.name = \"xÌ‚\"\n",
    "    zÌ‚ = dense(w2, xÌ‚, Ïƒ)\n",
    "    zÌ‚.name = \"zÌ‚\" \n",
    "    Å· = dense(w3, zÌ‚)\n",
    "    Å·.name = \"Å·\"\n",
    "    pred = return_prediction(yÌ‚)\n",
    "    pred.name = \"pred\"\n",
    "    return topological_sort(pred)\n",
    "end\n",
    "\n",
    "for b=1:3000:60000\n",
    "    x = Variable(trainX[:,:,b], name=\"x\")\n",
    "    y = Variable(trainY[b], name=\"y\")\n",
    "    result = predict(x, W1, W2, W3)\n",
    "    println(forward!(result)) #predict ma to samo co architektura bez funkcji strat, obliczana jest tutaj wartoÅ›Ä‡ bo fwd ma compute\n",
    "    println(trainY[b])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Gadfly: spy\n",
    "b=2354\n",
    "a=trainX[:,:,b]\n",
    "spy(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
